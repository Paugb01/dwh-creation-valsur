# Data Warehouse Ingestion Pipeline

A production-ready data pipeline for extracting data from MySQL databases and loading it into BigQuery with advanced processing strategies.

## ğŸ—ï¸ Architecture

This pipeline implements a modern data architecture with:
- **Bronze Layer**: Raw data storage in BigQuery
- **Silver Layer**: Processed and cleaned data
- **Gold Layer**: Analytics-ready datasets
- **Advanced Strategies**: Per-table processing optimization

## ğŸ“ Project Structure

```
batch_ingestion/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ .env.example                       # Environment template
â”œâ”€â”€ .gitignore                         # Git ignore rules
â”‚
â”œâ”€â”€ core/                              # Core pipeline modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ batch_extractor.py             # MySQL extraction logic
â”‚   â””â”€â”€ utils.py                       # Shared utilities
â”‚
â”œâ”€â”€ bigquery/                          # BigQuery ingestion components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py                        # Advanced BigQuery worker
â”‚
â”œâ”€â”€ config/                            # Configuration files
â”‚   â”œâ”€â”€ config.json                    # Main pipeline configuration
â”‚   â”œâ”€â”€ secrets.json                   # Credentials (gitignored)
â”‚   â””â”€â”€ secrets.json.template          # Secrets template
â”‚
â”œâ”€â”€ scripts/                           # Executable scripts
â”‚   â”œâ”€â”€ run_extraction.py              # Run data extraction
â”‚   â”œâ”€â”€ run_bigquery_ingestion.py      # Run BigQuery ingestion
â”‚   â””â”€â”€ migrate_region.py              # Region migration utility
â”‚
â”œâ”€â”€ tests/                             # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_pipeline_integration.py   # End-to-end pipeline tests
â”‚   â””â”€â”€ test_advanced_strategies.py    # Advanced strategy tests
â”‚
â”œâ”€â”€ orchestration/                     # Airflow DAGs and deployment
â”‚   â”œâ”€â”€ dags/                          # Airflow DAG definitions
â”‚   â”œâ”€â”€ config/                        # Airflow configuration
â”‚   â””â”€â”€ README.md                      # Orchestration documentation
â”‚
â”œâ”€â”€ docs/                              # Documentation
â”‚   â”œâ”€â”€ IMPLEMENTATION.md              # Technical implementation details
â”‚   â””â”€â”€ ADVANCED_STRATEGIES.md         # Advanced ingestion strategies
â”‚
â””â”€â”€ data/                              # Data storage (gitignored)
    â”œâ”€â”€ extracted/                     # Local extracted data
    â”œâ”€â”€ logs/                          # Pipeline logs
    â””â”€â”€ temp/                          # Temporary files
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone <repository-url>
cd batch_ingestion

# Create virtual environment
python -m venv .venv
.venv/Scripts/activate  # Windows
source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration

```bash
# Copy environment template
cp .env.example .env

# Copy secrets template
cp config/secrets.json.template config/secrets.json

# Edit configuration files
# - Update .env with your environment variables
# - Update config/secrets.json with your credentials
# - Modify config/config.json for your tables and strategies
```

### 3. Run the Pipeline

```bash
# Extract data from MySQL
python scripts/run_extraction.py

# Load data to BigQuery with advanced strategies
python scripts/run_bigquery_ingestion.py

# Run full integration test
python tests/test_pipeline_integration.py
```

## ğŸ”§ Configuration

### Main Configuration (`config/config.json`)

```json
{
  "bigquery": {
    "datasets": {
      "bronze": "bronze1",
      "silver": "silver1", 
      "gold": "gold1"
    },
    "location": "europe-southwest1",
    "source_database": "your_database",
    "target_tables": ["table1", "table2"]
  },
  "table_strategies": {
    "table1": {
      "strategy": "incremental_merge",
      "partition_field": "created_date",
      "merge_keys": ["id"]
    },
    "table2": {
      "strategy": "replace_partition", 
      "partition_field": "snapshot_date"
    }
  }
}
```

### Secrets Configuration (`config/secrets.json`)

```json
{
  "mysql": {
    "host": "your-mysql-host",
    "port": 3306,
    "database": "your_database",
    "username": "your_username",
    "password": "your_password"
  },
  "gcp": {
    "project_id": "your-gcp-project",
    "bucket_name": "your-gcs-bucket"
  }
}
```

## ğŸ“Š Advanced Processing Strategies

The pipeline supports multiple processing strategies optimized for different data types:

### 1. Incremental Merge
- **Use Case**: Transactional data with updates
- **How it Works**: Merges new/changed records based on merge keys
- **Best For**: Order history, transaction logs

### 2. Replace Partition
- **Use Case**: Daily snapshots or time-series data  
- **How it Works**: Replaces entire partition for the processing date
- **Best For**: Inventory snapshots, daily aggregations

### 3. SCD Type 1 Upsert
- **Use Case**: Master data that changes over time
- **How it Works**: Updates existing records or inserts new ones
- **Best For**: Customer data, product catalogs

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest tests/

# Run integration tests
python tests/test_pipeline_integration.py

# Run advanced strategy tests
python tests/test_advanced_strategies.py

# Test specific components
python -m pytest tests/test_extraction.py
```

## ğŸ“ˆ Performance

The pipeline is optimized for high-volume data processing:

- **Parallel Processing**: Multi-threaded extraction and loading
- **Batch Operations**: Optimized batch sizes for MySQL and BigQuery
- **Hive-Compatible Partitioning**: Efficient GCS storage layout
- **Strategy-Based Processing**: Per-table optimization reduces processing time

### Typical Performance Metrics
- **Extraction**: ~100K records/minute per table
- **BigQuery Loading**: ~500K records/minute
- **End-to-End**: 15-20 minutes for ~220 tables (~1.6M records)

## ğŸ”„ Orchestration

The pipeline includes Airflow DAGs for production orchestration:

```bash
# Deploy to Google Cloud Composer
cd orchestration
./deploy.sh

# Local Airflow development
./setup.sh
```

## ğŸŒ Multi-Region Support

Use the migration utility to move resources between regions:

```bash
python scripts/migrate_region.py
```

## ğŸ“š Documentation

- [Implementation Details](docs/IMPLEMENTATION.md)
- [Advanced Strategies](docs/ADVANCED_STRATEGIES.md)
- [Orchestration Guide](orchestration/README.md)

## ğŸ¤ Contributing

1. Follow the established project structure
2. Add tests for new features
3. Update documentation
4. Ensure all tests pass before submitting

## ğŸ“ License

This project is proprietary software for Valsur Truck data warehouse operations.

## ğŸ†˜ Support

For issues and questions:
1. Check the documentation in the `docs/` folder
2. Run the integration tests to validate your setup
3. Review the logs in the `data/logs/` directory
